{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOPJBGA8tnzkdFQbRGkl8fc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryu-Tamura/GoogleColab_ChatbotTest/blob/main/04_%E8%8B%B1%E4%BC%9A%E8%A9%B1%E3%83%81%E3%83%A3%E3%83%83%E3%83%88%E3%83%9C%E3%83%83%E3%83%88_MVP4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**MVP4: UI改善・画像・履歴表示**\n",
        "\n",
        "目的: 音声入出力、会話履歴（テキスト表示）、画像表示機能を持つ、シンプルで理解しやすいWebアプリを作成します。\n",
        "\n",
        "手順:\n",
        "\n",
        "1. 必要なライブラリをインストールします。\n",
        "2. ライブラリを読み込みます。\n",
        "3. OpenAI APIキーを設定します。\n",
        "4. 表示したい画像をアップロードします。\n",
        "5. 会話履歴を保存する変数 (`history`) を準備します。\n",
        "6. シンプルな音声認識関数を作成します。\n",
        "7. シンプルな音声合成関数を作成します。\n",
        "8. 新しいシンプルなメイン関数を作成します (履歴テキスト整形含む)。\n",
        "9. Gradioを使って、レイアウトを整え、画像と履歴(テキスト)を表示するUIを作ります (Blocks を使用)。\n",
        "10. Webアプリを起動します。\n",
        "\n",
        "-------------------------------\n",
        "\n",
        "1. ライブラリのインストール\n",
        "\n"
      ],
      "metadata": {
        "id": "BRfyxWG-R43e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOxzW2J5Hs5M"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------\n",
        "\n",
        "2. ライブラリのインポート\n",
        "\n"
      ],
      "metadata": {
        "id": "oAyIFgKKSJO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import datetime"
      ],
      "metadata": {
        "id": "boeOwE64IWnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------\n",
        "\n",
        "\n",
        "3. OpenAI APIキーの設定\n",
        "\n",
        "※事前にColabの左メニュー(🔑)で `OPENAI_API_KEY` という名前でAPIキーを設定しておいてください。"
      ],
      "metadata": {
        "id": "Imeyu5E3SMcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata # APIキーを安全に読み込むため\n",
        "key = userdata.get('OPENAI_API_KEY')\n",
        "client = openai.OpenAI(api_key=key)"
      ],
      "metadata": {
        "id": "9DPLfP2DIYPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------\n",
        "\n",
        "4. 表示する画像の準備"
      ],
      "metadata": {
        "id": "L0g5fpiBSSoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colabに画像ファイルをアップロードして変数に読み込む\n",
        "from google.colab import files\n",
        "\n",
        "print(\"表示する画像を1枚アップロードしてください...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "image_to_display = None # 初期値\n",
        "\n",
        "# アップロードされたファイル名とサイズを表示\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "# 最初のファイル名を取得し、Gradioで使えるファイルパスを作成\n",
        "filename = list(uploaded.keys())[0]\n",
        "image_to_display = os.path.abspath(filename) # 絶対パスを取得"
      ],
      "metadata": {
        "id": "L7tN6U7PIZfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------\n",
        "\n",
        "5. 会話履歴 (history) の準備\n",
        "\n",
        "※このセルはノートブックを開いて最初に一度だけ実行します。\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HbrCpsSISYWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# グローバル変数として会話履歴を初期化\n",
        "history = []\n",
        "\n",
        "# AIへの指示（システムプロンプト）を定義\n",
        "system_prompt = \"\"\"あなたは、ユーザーと自然で流暢な英会話を行うAI英語教師です。\n",
        "\n",
        "# あなたの役割\n",
        "- ユーザーの英語力向上のために、自然で日常的な英会話を行ってください。\n",
        "- ユーザーの英語表現や文法に間違いがあれば、会話の流れを止めないように、自然な形で指摘し、より適切な表現を提案してください。\n",
        "- ユーザーの質問や発言には、共感的に、そして前向きな態度で応答してください。\n",
        "\n",
        "# 回答の形式\n",
        "- 会話は常に英語で行ってください。日本語で応答してはいけません。\n",
        "- 回答は、原則として短く（1～3文程度）、自然な口語表現を使ってください。\n",
        "- ユーザーが文法や表現のミスをした場合は、以下のような形式で訂正例を示してください。\n",
        "  例 (Example): User: \"I goed to park yesterday.\" AI: \"Oh, nice! You mean, 'I went to the park yesterday,' right?\"\n",
        "\n",
        "# 入出力の例 (Input/Output Examples)\n",
        "User: \"Hello! How was your day?\" AI: \"Hey! My day's going great, thanks for asking. How about you?\"\n",
        "User: \"I'm study English today.\" AI: \"That's great! Just a quick tip: you'd say, 'I'm studying English today.' What are you working on right now?\"\n",
        "User: \"Can you help me about grammar?\" AI: \"Of course! I'd say, 'Can you help me with grammar?' Sure thing! What grammar point do you want to talk about?\" \"\"\"\n",
        "\n",
        "# historyリストの最初にシステムプロンプトを追加\n",
        "history.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "\n",
        "print(\"会話履歴(history)を初期化しました。\")"
      ],
      "metadata": {
        "id": "jKxxEYQTQRzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------\n",
        "\n",
        "\n",
        "6. シンプルな音声認識関数\n",
        "\n"
      ],
      "metadata": {
        "id": "Bje8fUI8Se6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def speech_to_text_simple(audio_filepath):\n",
        "    \"\"\"簡単な音声認識関数 (Whisper)\"\"\"\n",
        "    if audio_filepath is None: return \"\"\n",
        "    try:\n",
        "        audio_file = open(audio_filepath, \"rb\")\n",
        "        transcription = client.audio.transcriptions.create(\n",
        "            model=\"whisper-1\",\n",
        "            file=audio_file,\n",
        "            language=\"en\")\n",
        "        return transcription.text\n",
        "    except Exception as e:\n",
        "        print(f\"音声認識エラー: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "7F_t2fxIQfg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------\n",
        "\n",
        "\n",
        "7. シンプルな音声合成関数\n",
        "\n"
      ],
      "metadata": {
        "id": "4iAuNZW6SjT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_speech_simple(text, output_dir=\"tts_output_simple_v4\"): # 保存先フォルダ名変更\n",
        "    \"\"\"簡単な音声合成関数 (OpenAI TTS)\"\"\"\n",
        "    if not text: return None\n",
        "    try:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        speech_file_path = os.path.join(output_dir, f\"response_{timestamp}.mp3\")\n",
        "        response = client.audio.speech.create(model=\"tts-1\", voice=\"alloy\", input=text)\n",
        "        response.stream_to_file(speech_file_path)\n",
        "        return speech_file_path\n",
        "    except Exception as e:\n",
        "        print(f\"音声合成エラー: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "TZ4LHS5fQiFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------\n",
        "\n",
        "8. シンプルなメイン関数"
      ],
      "metadata": {
        "id": "uxf9CXl1SojN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def english_teacher_bot_final(input_mode, input_text, input_audio):\n",
        "    \"\"\"\n",
        "    入力に応じてAIと対話し、履歴を更新し、\n",
        "    入力テキスト、応答テキスト、応答音声パス、履歴テキストを返す関数。\n",
        "    \"\"\"\n",
        "    global history\n",
        "\n",
        "    # 1. 入力モードに応じてテキスト化 (簡単なフォールバック付き)\n",
        "    processed_input_text = \"\"\n",
        "    if input_mode == \"Text\":\n",
        "        processed_input_text = input_text\n",
        "    elif input_mode == \"Voice\":\n",
        "        if input_audio is not None:\n",
        "            try:\n",
        "                processed_input_text = speech_to_text_simple(input_audio)\n",
        "            except Exception as e: # 音声認識中の予期せぬエラー\n",
        "                 print(f\"音声認識中に予期せぬエラー: {e}\")\n",
        "                 processed_input_text = \"\" # エラー時は空にする\n",
        "            if not processed_input_text: # 認識失敗またはエラーの場合\n",
        "                print(\"音声認識失敗。テキスト入力があれば使います。\")\n",
        "                processed_input_text = input_text # テキスト入力でフォールバック\n",
        "        else: # Voiceモードで音声がない場合\n",
        "             print(\"音声モードですが音声がありません。テキスト入力があれば使います。\")\n",
        "             processed_input_text = input_text # テキスト入力でフォールバック\n",
        "\n",
        "    print(f\"処理された入力テキスト: {processed_input_text}\")\n",
        "\n",
        "    # 有効な入力がない場合は終了\n",
        "    if not processed_input_text:\n",
        "        # 戻り値の数を合わせる (履歴テキストは現在の履歴から生成)\n",
        "        history_display_text = \"--- 会話履歴 ---\\n\"\n",
        "        for msg in history:\n",
        "             if msg[\"role\"] == \"user\": history_display_text += f\"あなた: {msg['content']}\\n\"\n",
        "             elif msg[\"role\"] == \"assistant\": history_display_text += f\"AI: {msg['content']}\\n\\n\"\n",
        "        return \"(入力なし)\", \"(入力がなかったため応答できません)\", None, history_display_text\n",
        "\n",
        "    # 2. ユーザー入力を履歴に追加\n",
        "    history.append({\"role\": \"user\", \"content\": processed_input_text})\n",
        "\n",
        "    # 3. ChatGPT API呼び出し\n",
        "    assistant_response_text = \"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\", messages=history, max_tokens=100, temperature=0.7\n",
        "        )\n",
        "        assistant_response_text = response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        assistant_response_text = f\"エラー: {e}\"\n",
        "        print(assistant_response_text)\n",
        "        history.pop() # エラー時はユーザー入力削除\n",
        "\n",
        "    # 4. AI応答を履歴に追加\n",
        "    history.append({\"role\": \"assistant\", \"content\": assistant_response_text})\n",
        "    print(f\"AI応答テキスト: {assistant_response_text}\")\n",
        "\n",
        "    # 5. 音声合成\n",
        "    response_audio_path = None\n",
        "    if not assistant_response_text.startswith(\"エラー\"):\n",
        "        response_audio_path = text_to_speech_simple(assistant_response_text)\n",
        "\n",
        "    # 6. 履歴表示用テキストを作成 (毎回全履歴を整形)\n",
        "    history_display_text = \"--- 会話履歴 ---\\n\"\n",
        "    # システムプロンプトを除いて表示する場合 (オプション)\n",
        "    # for msg in history[1:]: # インデックス1から開始\n",
        "    for msg in history: # システムプロンプトも含めて表示する場合\n",
        "        prefix = \"\"\n",
        "        if msg[\"role\"] == \"system\": prefix = \"システム設定:\\n\"\n",
        "        elif msg[\"role\"] == \"user\": prefix = f\"あなた ({input_mode}): \" # 入力モードも表示\n",
        "        elif msg[\"role\"] == \"assistant\": prefix = \"AI: \"\n",
        "        history_display_text += f\"{prefix}{msg['content']}\\n\\n\" # メッセージ間に空行\n",
        "\n",
        "    # 7. 結果を返す (入力テキスト, 応答テキスト, 応答音声パス, 履歴テキスト)\n",
        "    return processed_input_text, assistant_response_text, response_audio_path, history_display_text\n",
        "\n",
        "print(\"メイン関数 'english_teacher_bot_final' を定義しました。\")"
      ],
      "metadata": {
        "id": "8h68xj_yQkyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------\n",
        "\n",
        "9. Gradio UI 作成 (Blocks、画像、履歴テキスト表示)\n",
        "\n"
      ],
      "metadata": {
        "id": "UEJg1SCrSvVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gr.Blocks を使ってUIを作成します\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"# 英会話チャットボット\") # タイトル統一\n",
        "    gr.Markdown(\"テキストまたは音声で入力し、「送信」ボタンを押してください。\")\n",
        "\n",
        "    # UIを左右2列に分割\n",
        "    with gr.Row():\n",
        "        # 左列: 入力と履歴表示\n",
        "        with gr.Column(scale=2): # 左を広めに\n",
        "            gr.Markdown(\"### 入力\")\n",
        "            input_mode = gr.Radio([\"Text\", \"Voice\"], label=\"入力方法\", value=\"Text\")\n",
        "            input_text = gr.Textbox(label=\"テキスト入力\", placeholder=\"または、下で録音\")\n",
        "            input_audio = gr.Audio(sources=[\"microphone\"], type=\"filepath\", label=\"音声入力\")\n",
        "            submit_button = gr.Button(\"送信\", variant=\"primary\")\n",
        "            gr.Markdown(\"### 会話履歴\")\n",
        "            # 履歴表示用の複数行テキストボックス (読み取り専用にしない場合は interactive=True)\n",
        "            history_display = gr.Textbox(label=\"会話ログ\", lines=15, interactive=False)\n",
        "\n",
        "        # 右列: 出力と画像\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 応答 / 結果\")\n",
        "            # 戻り値1 (processed_input_text) を表示 (音声認識結果の確認用)\n",
        "            recognized_text = gr.Textbox(label=\"入力内容\", interactive=False)\n",
        "            # 戻り値2 (assistant_response_text) を表示\n",
        "            output_text = gr.Textbox(label=\"AIの応答 (テキスト)\", interactive=False, lines=4)\n",
        "            # 戻り値3 (response_audio_path) を表示\n",
        "            output_audio = gr.Audio(label=\"AIの応答 (音声)\", type=\"filepath\", interactive=False)\n",
        "            gr.Markdown(\"### 画像\")\n",
        "            # 画像表示\n",
        "            display_image = gr.Image(value=image_to_display, label=\"表示画像\", interactive=False)\n",
        "\n",
        "    # --- イベントリスナーの設定 ---\n",
        "    submit_button.click(\n",
        "        fn=english_teacher_bot_final, # 作成したメイン関数\n",
        "        # 入力: モード, テキスト, 音声 (履歴はグローバル変数)\n",
        "        inputs=[input_mode, input_text, input_audio],\n",
        "        # 出力: 認識テキスト, 応答テキスト, 応答音声, **履歴表示テキスト**\n",
        "        outputs=[recognized_text, output_text, output_audio, history_display]\n",
        "    )\n",
        "    # テキスト入力時のEnterキー送信も同様に設定 (オプション)\n",
        "    # input_text.submit(...)\n",
        "\n",
        "print(\"Gradioインターフェースを作成しました。\")"
      ],
      "metadata": {
        "id": "_VoqUPeRQpzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------\n",
        "\n",
        "10. Webアプリの起動\n",
        "\n"
      ],
      "metadata": {
        "id": "5KWKsP7NSz9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 作成したUIを起動します\n",
        "app.launch(share=True)"
      ],
      "metadata": {
        "id": "L0Z6yBAFIi5M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}